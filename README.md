# NLP_Seq_To_Seq_Transformers
This project explores machine translation using both LSTM-based Seq2Seq models and Transformer architectures. Using the Multi30K dataset as a primary example, it walks through the end-to-end pipelineâ€”from tokenization and vocabulary creation to training and evaluating neural models for English-German translation. It also includes an attention mechanism and visual comparisons of predictions from different models.

Additionally, the repo demonstrates how Transformer encoders can be applied to grammaticality tasks using the CoLA dataset, serving as a sanity check for the architecture. With custom implementations and PyTorch-based training loops, this notebook offers a hands-on, visual, and educational approach to understanding modern NLP translation models.
