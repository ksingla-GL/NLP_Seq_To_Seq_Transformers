# NLP_Seq_To_Seq_Transformers
This project implements neural machine translation in two ways. One model is a classic LSTM-based Seq2Seq (encoder–decoder) architecture with an attention mechanism. The other is a modern Transformer model that uses self-attention for fast, parallel training. Both models are trained on the Multi30K dataset – a popular English–German benchmark of about 30,000 sentence pairs. All components (encoder, decoder, attention, etc.) are built from scratch in clear Python code, and the repo includes both the training routines and inference scripts so you can easily translate new sentences.

I also include a simple grammar acceptability task as a sanity check. The Transformer encoder is trained on the CoLA dataset (Corpus of Linguistic Acceptability), which contains roughly 10,000 English sentences labeled by linguists for grammatical correctness. The model learns to predict whether a sentence is grammatically acceptable or not, demonstrating that the same Transformer code can handle a basic classification task. Everything is written in an accessible, step-by-step style with clear examples.
